TPCâ€‘DS ML Pipeline on Databricks
Endâ€‘toâ€‘End Machine Learning Project with PySpark, MLflow, and Databricks Workflows
This project demonstrates a complete machine learning workflow on Databricks, using the TPCâ€‘DS SF1000 dataset as the foundation for largeâ€‘scale feature engineering and model development. It highlights modern ML engineering practices including:

Distributed data processing with PySpark

Feature engineering on Delta Lake

Experiment tracking with MLflow

Model lifecycle management with the Databricks Model Registry

Orchestration using Databricks Workflows

Optional: Databricks Feature Store + Model Serving

This repository is designed as a portfolioâ€‘ready example of how to build productionâ€‘grade ML pipelines on Databricks.

ğŸ“ Project Structure
Code
project/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_ingest_data.py
â”‚   â”œâ”€â”€ 02_feature_engineering.py
â”‚   â”œâ”€â”€ 03_train_model.py
â”‚   â”œâ”€â”€ 04_register_and_deploy.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ utils.py
|
â”œâ”€â”€ tests/
|   â”œâ”€â”€ __init__.py
|   â”œâ”€â”€ test_utils.py
â”‚
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ config.yaml
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ“¦ Dataset: TPCâ€‘DS SF1000
The TPCâ€‘DS dataset is a wellâ€‘known industry benchmark for decision support systems. Databricks provides a preâ€‘generated scale factor 1000 dataset (tpcds_sf1000) containing billions of rows across dozens of tables.

This project uses a subset of tables relevant to customer purchasing behavior, such as:

store_sales

customer

item

date_dim

These tables are ideal for demonstrating largeâ€‘scale feature engineering with Spark.

ğŸ¯ Project Goal
Predict customer spending behavior using historical store sales and customer attributes.

Example target variable:

Total spend per customer over a defined time window
or

Probability a customer exceeds a spend threshold

This is flexible â€” the goal is to demonstrate the ML engineering workflow, not optimize a specific benchmark.

ğŸ§± Architecture Overview
Code
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   TPCâ€‘DS SF1000 Dataset   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Ingest & Bronze Tables  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Feature Engineering      â”‚
                â”‚  (PySpark â†’ Delta Lake)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Model Training        â”‚
                â”‚   (MLflow Experiments)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Model Registry (Prod)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Batch Scoring / Serving   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ§ª 1. Data Ingestion
Notebook: 01_ingest_data.py

Reads TPCâ€‘DS tables from Databricks datasets

Writes them as Delta tables in the workspace

Ensures schema consistency and partitioning

Example:

python
df = spark.read.table("tpcds_sf1000_delta.store_sales")
df.write.format("delta").mode("overwrite").saveAsTable("ml_tpcds.store_sales_bronze")
ğŸ”§ 2. Feature Engineering (PySpark)
Notebook: 02_feature_engineering.py

Key transformations include:

Joining customer, item, and sales tables

Aggregating spend metrics

Creating temporal features (day of week, seasonality)

Handling missing values

Writing feature tables to Delta

Example:

python
features = (
    sales.join(customers, "customer_id")
         .groupBy("customer_id")
         .agg(
             sum("sales_price").alias("total_spend"),
             count("*").alias("num_transactions"),
             avg("quantity").alias("avg_quantity")
         )
)
features.write.format("delta").mode("overwrite").saveAsTable("ml_tpcds.features")
ğŸ¤– 3. Model Training with MLflow
Notebook: 03_train_model.py

This notebook demonstrates:

Converting Spark features to Pandas or using Spark MLlib

Logging parameters, metrics, and artifacts

Tracking multiple experiments

Example:

python
with mlflow.start_run():
    model = RandomForestRegressor(n_estimators=200)
    model.fit(X_train, y_train)

    mlflow.log_param("n_estimators", 200)
    mlflow.log_metric("rmse", rmse)
    mlflow.sklearn.log_model(model, "model")
ğŸ“š 4. Model Registry Integration
Notebook: 04_register_and_deploy.py

Registers the best model in the MLflow Model Registry

Transitions it to Staging or Production

Optionally enables Databricks Model Serving

Example:

python
registered = mlflow.register_model(
    model_uri=f"runs:/{run_id}/model",
    name="tpcds_customer_spend_model"
)
âš™ï¸ 5. Workflow Orchestration
A Databricks Workflow (Job) orchestrates the pipeline:

Ingest Data

Feature Engineering

Train Model

Register Model

Batch Scoring (optional)

This demonstrates productionâ€‘grade automation.

ğŸš€ Optional Enhancements
Feature Store
Register features for reuse across models.

Model Serving
Deploy the model as a REST endpoint.

Unity Catalog Integration
Store models, tables, and features with governance.

ğŸ“Š Results & Artifacts
The project produces:

MLflow experiment runs

Registered models with versioning

Delta feature tables

Workflow DAG screenshots

Model performance metrics

These artifacts are ideal for showcasing ML engineering skills.

ğŸ“ How to Run
Clone this repo into Databricks Repos

Attach a cluster with:

Runtime: DBR 13+ ML recommended

Autoscaling enabled

Run notebooks in order:

01_ingest_data.py

02_feature_engineering.py

03_train_model.py

04_register_and_deploy.py

(Optional) Create a Databricks Workflow to automate the pipeline

ğŸ“Œ Summary
This project demonstrates a full ML lifecycle on Databricks using a largeâ€‘scale dataset (TPCâ€‘DS SF1000). It highlights:

Distributed feature engineering

MLflow experiment tracking

Model registry best practices

Workflow orchestration

Productionâ€‘ready ML engineering patterns

It is designed to serve as a strong portfolio piece for transitioning into Machine Learning roles.