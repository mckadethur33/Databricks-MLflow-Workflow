{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51afa4dc-d18e-46a0-b6e9-507116acd64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # 01 â€” Data Ingestion (TPCâ€‘DS SF1000)\n",
    "# MAGIC \n",
    "# MAGIC This notebook ingests selected TPCâ€‘DS SF1000 tables into managed Delta tables.\n",
    "# MAGIC \n",
    "# MAGIC **Goals**\n",
    "# MAGIC - Load raw TPCâ€‘DS tables from Databricks datasets\n",
    "# MAGIC - Write them into your project schema as Delta (Bronze layer)\n",
    "# MAGIC - Apply basic schema validation and logging\n",
    "# MAGIC - Prepare data for downstream feature engineering\n",
    "# MAGIC \n",
    "# MAGIC **Tables used**\n",
    "# MAGIC - `store_sales`\n",
    "# MAGIC - `customer`\n",
    "# MAGIC - `item`\n",
    "# MAGIC - `date_dim`\n",
    "# MAGIC \n",
    "# MAGIC These tables support customerâ€‘level spend prediction.\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08b3928-2ace-4367-9c25-90482348d138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"workspace\"\n",
    "schema = \"ml_tpcds\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104ac07c-6a63-4496-9ff4-758c0c4d3d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Your project schema (Unity Catalog or Hive Metastore)\n",
    "catalog = \"workspace\"\n",
    "schema = \"ml_tpcds\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "print(f\"Using schema: {catalog}.{schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4ff073-4312-4ec7-9d54-5354705237af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Helper: Load TPCâ€‘DS table and write as Delta\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def ingest_table(source_table: str, target_table: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a TPCâ€‘DS table and writes it as a Delta table.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“¥ Ingesting {source_table} â†’ {target_table}\")\n",
    "\n",
    "    df = spark.read.table(source_table)\n",
    "\n",
    "    # Basic validation\n",
    "    if df.count() == 0:\n",
    "        raise ValueError(f\"Source table {source_table} is empty.\")\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Wrote {df.count():,} rows to {target_table}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94affa42-73fa-40bf-865d-696c8ee1a596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Ingest Raw TPCâ€‘DS Tables (Bronze Layer)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "tables_to_ingest = {\n",
    "    \"samples.tpcds_sf1000.store_sales\": f\"{catalog}.{schema}.store_sales_bronze\",\n",
    "    \"samples.tpcds_sf1000.customer\": f\"{catalog}.{schema}.customer_bronze\",\n",
    "    \"samples.tpcds_sf1000.item\": f\"{catalog}.{schema}.item_bronze\",\n",
    "    \"samples.tpcds_sf1000.date_dim\": f\"{catalog}.{schema}.date_dim_bronze\",\n",
    "}\n",
    "\n",
    "bronze_tables = {}\n",
    "\n",
    "for source, target in tables_to_ingest.items():\n",
    "    bronze_tables[target] = ingest_table(source, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9597828c-3188-447a-b180-c2ab77c35df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quick Profiling\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for table_name, df in bronze_tables.items():\n",
    "    print(f\"\\nðŸ“Š {table_name}\")\n",
    "    df.printSchema()\n",
    "    display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da353b7e-86b4-4c34-b6cc-c8ab2bdaf8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## MLflow Logging\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "with mlflow.start_run(run_name=\"data_ingestion_tpcds\"):\n",
    "    for table_name, df in bronze_tables.items():\n",
    "        mlflow.log_param(f\"{table_name}_rows\", df.count())\n",
    "        mlflow.log_param(f\"{table_name}_columns\", len(df.columns))\n",
    "\n",
    "    mlflow.log_param(\"catalog\", catalog)\n",
    "    mlflow.log_param(\"schema\", schema)\n",
    "\n",
    "print(\"Ingestion metadata logged to MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a9f01d-2d7c-493f-9d37-57cbb8b3498d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Ingestion Complete\n",
    "# MAGIC \n",
    "# MAGIC Your Bronze tables are now ready for feature engineering in the next notebook.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
